{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Agent-based Literature Curation","text":""},{"location":"#what-is-this","title":"What is this?","text":"<p>Validation of AI Agent-Based Curation for Low-Noise Literature Set Construction</p> <p>This repository provides complete research materials for validating AI agent-based literature curation methods, including:</p> <ul> <li>Comprehensive research documentation and findings</li> <li>Full analysis scripts and implementation code</li> <li>Experimental datasets and evaluation results</li> <li>Docker environment for reproducibility</li> </ul> <p>Built with Biomni: This research is built upon Biomni, an open-source AI agent framework developed at Stanford University for biological and biomedical research.</p> <p>Huang, K., Zhang, S., Wang, H., et al. (2025). Biomni: A General-Purpose Biomedical AI Agent. bioRxiv. https://doi.org/10.1101/2025.05.30.656746</p>"},{"location":"#methodology","title":"Methodology","text":"<p>We tested three different approaches for literature curation and validated their performance by comparing Precision scores:</p> <ul> <li>Default Biomni: A Biomni AI agent with default configuration</li> <li>Custom Biomni: A Biomni AI agent with custom enhancements</li> <li>PubTator: NCBI's annotation system</li> </ul> <p>Performance Metrics</p> <p>Currently validated using Precision only. Other metrics such as Accuracy and F1-score are not yet evaluated.</p> <p>For detailed research documentation, please refer to:</p> <ul> <li>BACKGROUND - Research context and motivation</li> <li>METHODS - Detailed methodology and operational definitions</li> <li>RESULTS - Performance comparison and metrics</li> <li>DISCUSSION - Analysis of false positives/negatives</li> <li>LIMITATIONS - Study constraints and future work</li> </ul>"},{"location":"#conference-presentation","title":"Conference Presentation","text":"<p>Presented at: The 48th Annual Meeting of the Molecular Biology Society of Japan</p> <p>Date: December 3, 2025</p>"},{"location":"#presention-material","title":"Presention Material","text":"<p>Download PDF</p>"},{"location":"#reproducibility","title":"Reproducibility","text":"<p>All data and scripts needed to reproduce this research are available in this GitHub repository.</p> <p>Repository: github.com/science-aid/agent-based-literature-curation</p> <p>Feel free to reference and use these materials for your own research.</p>"},{"location":"#contact","title":"Contact","text":"<p>Takayuki Suzuki \ud83d\udce7 takayuki.suzuki@science-aid.com \ud835\udd4f @sci_aid_tszk</p>"},{"location":"research/BACKGROUND/","title":"Background","text":""},{"location":"research/BACKGROUND/#research-context","title":"Research Context","text":"<p>The systematic identification and curation of genetic research literature on non-model organisms presents substantial practical challenges. While model organisms\u2014such as Escherichia coli, Drosophila melanogaster, and Mus musculus\u2014have long served as cornerstones of genetic research, studies of non-model organisms offer complementary biological insights that are not fully captured by these established systems.</p>"},{"location":"research/BACKGROUND/#scientific-importance-of-non-model-organism-genetics","title":"Scientific Importance of Non-Model Organism Genetics","text":"<p>Non-model organisms play a critical role across multiple domains of biological research.</p>"},{"location":"research/BACKGROUND/#biodiversity-and-functional-diversity","title":"Biodiversity and Functional Diversity","text":"<p>Non-model organisms often harbor unique metabolic pathways, adaptive strategies, and protein repertoires that are absent from conventional model systems. These features are essential for advancing evolutionary biology, ecology, and systems-level understanding of biological complexity.</p>"},{"location":"research/BACKGROUND/#applied-biotechnology-potential","title":"Applied Biotechnology Potential","text":"<p>Many non-model organisms represent largely untapped reservoirs of biotechnologically valuable traits, including novel enzymes, natural product biosynthesis pathways, and mechanisms of tolerance to environmental stresses such as salinity, drought, and disease.</p>"},{"location":"research/BACKGROUND/#phylogenetics-and-comparative-genomics","title":"Phylogenetics and Comparative Genomics","text":"<p>Accurate inference of phylogenetic relationships and gene family evolution requires broad taxonomic sampling. Overreliance on model organisms constrains the resolution and generalizability of comparative genomic analyses.</p>"},{"location":"research/BACKGROUND/#addressing-taxonomic-bias","title":"Addressing Taxonomic Bias","text":"<p>The disproportionate emphasis on model organisms introduces systematic taxonomic bias into the scientific literature. Incorporation of non-model organisms enables validation of general biological principles and promotes a more representative understanding of biological systems.</p>"},{"location":"research/BACKGROUND/#problem-statement","title":"Problem Statement","text":"<p>Despite their scientific importance, the identification of genetic research focused on non-model organisms remains challenging.</p>"},{"location":"research/BACKGROUND/#current-limitations","title":"Current Limitations","text":""},{"location":"research/BACKGROUND/#labor-intensive-manual-curation","title":"Labor-Intensive Manual Curation","text":"<p>To our knowledge, no fully automated or systematic method currently exists for efficiently identifying non-model organism genetics papers from large-scale literature databases. Existing approaches rely heavily on manual review, including:</p> <ul> <li>Individual examination of titles, abstracts, and MeSH terms across thousands of publications (e.g., 4,940 papers in our case)</li> <li>Construction of complex database queries requiring expert knowledge of biological nomenclature</li> <li>Iterative searches across multiple resources (e.g., NCBI Gene, NCBI Taxonomy)</li> <li>Manual extraction and structuring of gene names, gene identifiers, species names, taxonomy IDs, higher-level taxonomic classifications, and research categories</li> </ul>"},{"location":"research/BACKGROUND/#complexity-beyond-traditional-workflows","title":"Complexity Beyond Traditional Workflows","text":"<p>Literature screening for non-model organism genetics involves challenges that exceed the capacity of traditional mechanical workflows, including:</p> <ul> <li>Multiple decision points requiring domain-specific expertise</li> <li>Autonomous task execution with adaptive query formulation</li> <li>Integration of heterogeneous biological databases</li> <li>Contextual interpretation that cannot be reliably captured by rule-based or simple keyword-matching approaches</li> </ul> <p>These characteristics motivate the need for more sophisticated, AI-driven solutions.</p>"},{"location":"research/BACKGROUND/#motivation","title":"Motivation","text":""},{"location":"research/BACKGROUND/#hypothesis","title":"Hypothesis","text":"<p>While PubTator demonstrates strong performance in species recognition and gene annotation for model organisms, we hypothesize that gene annotation for non-model organisms is inherently more challenging. This difficulty likely arises from heterogeneous and inconsistent gene nomenclature, resulting in suboptimal annotation performance.</p>"},{"location":"research/BACKGROUND/#validation-of-species-recognition-performance","title":"Validation of Species Recognition Performance","text":"<p>To contextualize this hypothesis, we examined the composition of PubTator\u2019s species recognition evaluation datasets to assess representation of non-model organisms:</p> <ul> <li>Analysis of the PubTator3 evaluation dataset revealed that 85.93% (104/635) of test cases correspond to non-model organisms (AIONER test dataset: https://github.com/ncbi/AIONER/blob/main/data/pubtator/SPECIES_Test.PubTator)</li> <li>Species named entity recognition and entity linking achieved high performance in these evaluations</li> <li>Validation results are documented in <code>data/species_match</code>, using <code>scripts/check_species_match.py</code></li> </ul> <p>Working Assumption Based on this evidence, we assume that PubTator\u2019s species recognition performance is reliable for non-model organisms and therefore focus our investigation on potential limitations in gene annotation.</p>"},{"location":"research/BACKGROUND/#ai-agent-based-approach","title":"AI Agent-Based Approach","text":"<p>Given the complexity of multi-step decision-making, database integration, and contextual interpretation required for literature curation, we propose an AI agent-based approach as a suitable solution. AI agents are capable of:</p> <ul> <li>Autonomously navigating complex decision trees during literature evaluation</li> <li>Performing iterative database queries with adaptive refinement</li> <li>Integrating information across multiple biological databases</li> <li>Extracting and structuring relevant metadata, including gene identifiers, species information, taxonomic classifications, and research categories</li> <li>Scaling to large literature corpora while maintaining consistent evaluation criteria</li> </ul>"},{"location":"research/BACKGROUND/#related-work","title":"Related Work","text":""},{"location":"research/BACKGROUND/#existing-literature-annotation-systems","title":"Existing Literature Annotation Systems","text":"<p>PubTator PubTator, developed by the NCBI, provides automated bioconcept annotation for biomedical literature. Although effective for many annotation tasks, its performance in gene annotation for non-model organisms has not been systematically evaluated.</p> <p>Manual Curation Gold-standard annotations depend on expert manual curation, which remains prohibitively time-consuming and impractical for large-scale literature databases.</p> <p>Rule-Based Systems Traditional keyword-based and rule-based approaches lack the flexibility and contextual understanding required to accurately identify non-model organism genetics research.</p>"},{"location":"research/BACKGROUND/#gap-in-current-approaches","title":"Gap in Current Approaches","text":"<p>To date, no existing system simultaneously provides:</p> <ol> <li>High-precision identification of non-model organism genetics literature</li> <li>Automated and scalable processing of large literature corpora</li> <li>Structured extraction of genetic and taxonomic metadata</li> <li>Domain-aware decision-making for ambiguous or borderline cases</li> </ol> <p>This study addresses these gaps through the development and evaluation of AI agent-based literature curation systems specifically tailored to non-model organism genetics research.</p>"},{"location":"research/DISCUSSION/","title":"Discussion","text":""},{"location":"research/DISCUSSION/#overview","title":"Overview","text":"<p>This study demonstrates that AI agent\u2013based approaches, particularly when combined with database-aware reasoning, can substantially outperform existing automated literature annotation systems in constructing high-precision corpora of genetic research articles on non-model organisms. Custom Biomni achieved 90.91% precision, representing a 30.91 percentage point improvement over PubTator (60.00%) and a 10.91 percentage point improvement over Default Biomni (80.00%).</p> <p>Beyond performance differences, our analysis reveals how agentic tool use, prompt design, and database coverage collectively shape the observed precision\u2013recall trade-offs.</p>"},{"location":"research/DISCUSSION/#comparative-performance-analysis","title":"Comparative Performance Analysis","text":""},{"location":"research/DISCUSSION/#why-custom-biomni-outperforms-other-methods","title":"Why Custom Biomni Outperforms Other Methods","text":"<p>Three primary factors account for Custom Biomni\u2019s superior precision.</p>"},{"location":"research/DISCUSSION/#1-enhanced-entity-linking-via-database-integration","title":"1. Enhanced Entity Linking via Database Integration","text":"<p>Custom Biomni integrates external biomedical databases through an MCP server, enabling real-time access to NCBI Gene and Taxonomy resources. This allows:</p> <ul> <li>Gene validation against official records and synonyms</li> <li>Species verification using authoritative taxonomy identifiers</li> <li>Species\u2013gene consistency checks to confirm biological plausibility</li> </ul> <p>Impact: By autonomously querying MCP-connected databases during inference, Custom Biomni performs context-aware entity grounding rather than static post-processing. This agentic tool use enables disambiguation of gene and species mentions based on biological plausibility\u2014for example, distinguishing biomarker proteins from genes or rejecting invalid gene\u2013species associations. As a result, false positives arising from superficial or ambiguous co-mentions are substantially reduced.</p> <p>Contrast with PubTator: PubTator relies on pre-computed NER annotations and rule-based filtering, without the ability to dynamically verify whether a gene mention represents the focus of genetic investigation or merely incidental background information.</p> <p>Contrast with Default Biomni: Default Biomni can access some external databases, including NCBI Gene and Taxonomy, but such access is intermittent and not reliably integrated into the inference process, limiting consistent context-aware validation.</p>"},{"location":"research/DISCUSSION/#2-structured-output-generation","title":"2. Structured Output Generation","text":"<p>Custom Biomni employs structured output tools that incrementally persist validated annotations to external files. This provides:</p> <ul> <li>Reliability, preserving outputs even when agent execution is interrupted</li> <li>Consistency, through enforced output schemas</li> <li>Auditability, enabling traceability of agent decisions</li> </ul> <p>By contrast, Default Biomni produced heterogeneous outputs requiring manual extraction from logs. Of 531 processed papers, only 387 yielded readily parseable outputs, increasing post-processing effort and reducing practical usability.</p>"},{"location":"research/DISCUSSION/#3-contextual-understanding-via-llm-reasoning","title":"3. Contextual Understanding via LLM Reasoning","text":"<p>Both Biomni variants leverage LLM-based reasoning to interpret document-level context, enabling:</p> <ul> <li>Identification of primary research intent</li> <li>Discrimination between genetic research and incidental gene mentions</li> <li>Iterative resolution of ambiguous cases</li> </ul> <p>This capability explains why both Biomni variants eliminated entire classes of false positives that were frequent in PubTator outputs.</p>"},{"location":"research/DISCUSSION/#false-positive-analysis","title":"False Positive Analysis","text":""},{"location":"research/DISCUSSION/#custom-biomni","title":"Custom Biomni","text":"<p>Despite high precision, Custom Biomni produced four false positives, revealing two dominant error modes:</p> <ol> <li>Gene vs. gene product ambiguity, where proteins, biomarkers, or peptide hormones were incorrectly treated as genetic research targets</li> <li>Model organism contamination, particularly in multi-organism studies where human genes were expressed or discussed in non-model systems</li> </ol> <p>These cases highlight semantic boundaries that remain challenging even for expert curators.</p>"},{"location":"research/DISCUSSION/#default-biomni","title":"Default Biomni","text":"<p>Default Biomni produced 15 false positives, reflecting broader and more systematic issues:</p> <ul> <li>Inclusion of papers with no substantive genetic content</li> <li>Insufficient filtering of model organisms</li> <li>Acceptance of tangential enzyme or gene mentions</li> </ul> <p>These errors stem largely from overly inclusive classification criteria rather than misidentification of specific entities.</p>"},{"location":"research/DISCUSSION/#pubtator","title":"PubTator","text":"<p>PubTator produced 22 false positives, dominated by two patterns:</p> <ol> <li>Human gene misclassification, where papers primarily focused on human genetics but mentioned non-model organisms</li> <li>False gene annotations, including biomarkers, metabolites, and non-genetic entities labeled as genes</li> </ol> <p>These errors reflect fundamental limitations of entity-level annotation without document-level reasoning.</p>"},{"location":"research/DISCUSSION/#false-negative-analysis","title":"False Negative Analysis","text":""},{"location":"research/DISCUSSION/#conservative-classification-in-custom-biomni","title":"Conservative Classification in Custom Biomni","text":"<p>Custom Biomni retrieved fewer papers overall (44) than Default Biomni (75), indicating reduced recall. This difference arises from two interacting factors.</p> <p>First, the precision-oriented design introduces an inherent trade-off. More importantly, prompt design amplified this effect. The instruction to always verify species\u2013gene consistency using NCBI databases imposed an overly strict constraint for non-model organisms, whose genes are often absent from curated databases. Consequently, legitimate genetic studies lacking registered NCBI Gene entries were systematically excluded.</p> <p>Second, Custom Biomni applied a stricter interpretation of what constitutes \u201cgenetic research,\u201d excluding borderline studies that Default Biomni retained. Some of these cases likely represent genuine classification ambiguities rather than clear errors.</p>"},{"location":"research/DISCUSSION/#implications","title":"Implications","text":"<p>These findings indicate that Custom Biomni\u2019s false negatives are driven not only by model limitations but also by design choices\u2014particularly prompt constraints that do not fully account for incomplete database coverage in non-model organism research.</p>"},{"location":"research/DISCUSSION/#mechanistic-insights","title":"Mechanistic Insights","text":""},{"location":"research/DISCUSSION/#why-database-integration-improves-precision","title":"Why Database Integration Improves Precision","text":"<p>Database integration improves precision through synonym resolution, species\u2013gene consistency checks, and increased confidence in established genetic entities. However, these benefits depend on database completeness\u2014an assumption that does not hold uniformly for non-model organisms.</p>"},{"location":"research/DISCUSSION/#why-llm-reasoning-outperforms-traditional-ner-pipelines","title":"Why LLM Reasoning Outperforms Traditional NER Pipelines","text":"<p>PubTator\u2019s Fundamental Constraint: PubTator employs dedicated machine learning\u2013based NER models trained on curated biomedical corpora. These datasets could be biased toward human and other model organism genes, with limited representation of non-model organism genetics. As a result, performance degrades in non-model contexts, and annotations are often driven by string-level matches rather than research relevance.</p> <p>More fundamentally, PubTator\u2019s pipeline lacks document-level reasoning and cannot assess whether a recognized gene mention constitutes the focus of genetic investigation or merely incidental context. This limitation explains the high frequency of false positives observed in this study.</p> <p>In contrast, LLM-based agents integrate entity recognition with contextual interpretation, enabling intent recognition and research-type classification that are critical for high-precision curation.</p>"},{"location":"research/DISCUSSION/#precisionrecall-trade-off-and-design-implications","title":"Precision\u2013Recall Trade-off and Design Implications","text":"<p>A clear trade-off emerges:</p> <ul> <li>Custom Biomni: Highest precision, lower recall due to strict validation requirements</li> <li>Default Biomni: Moderate precision with higher recall</li> <li>PubTator: Lowest precision, with recall dependent on annotation coverage</li> </ul> <p>Optimal method selection therefore depends on use case, ranging from high-precision curation to exploratory literature discovery.</p>"},{"location":"research/DISCUSSION/#broader-implications","title":"Broader Implications","text":"<p>This study demonstrates that AI agent\u2013based literature curation can approach manual-level precision while retaining scalability. At the same time, it highlights challenges specific to non-model organism research, particularly incomplete database coverage and ambiguous research boundaries.</p> <p>Importantly, the observed limitations point to clear improvement pathways: relaxed validation prompts, expanded non-model organism gene databases, and more capable reasoning models. Together, these advances could plausibly push precision beyond 95% while improving recall.</p>"},{"location":"research/DISCUSSION/#conclusion","title":"Conclusion","text":"<p>AI agent\u2013based literature curation with database integration substantially outperforms existing automated systems for identifying genetic research on non-model organisms. While Custom Biomni achieves high precision, its limitations underscore the importance of prompt design and database coverage in shaping performance. Overall, this work establishes both the feasibility of agent-based curation and a concrete roadmap toward more comprehensive, high-precision literature organization across the tree of life.</p>"},{"location":"research/DISCUSSION/#supplementary-information","title":"Supplementary Information","text":""},{"location":"research/DISCUSSION/#supplementary-note-1-detailed-false-positive-analysis","title":"Supplementary Note 1: Detailed False Positive Analysis","text":""},{"location":"research/DISCUSSION/#s11-custom-biomni-false-positives-4-cases","title":"S1.1 Custom Biomni False Positives (4 cases)","text":"<p>Despite achieving high precision, Custom Biomni produced four false positives, revealing specific edge-case error modes.</p>"},{"location":"research/DISCUSSION/#case-s111-pmid-39620069-hivhuman-evolutionary-study","title":"Case S1.1.1: PMID 39620069 \u2014 HIV\u2013Human Evolutionary Study","text":"<p>Description: Evolutionary analysis of the HIV gp120 gene in human populations. Error Type: Model organism contamination. Explanation: Although the agent correctly identified a gene-level study, it failed to exclude the paper due to primary involvement of a model organism (human). The presence of a non-model organism (HIV) led to misclassification.</p>"},{"location":"research/DISCUSSION/#case-s112-pmid-39624363-canine-crp-biomarker-review","title":"Case S1.1.2: PMID 39624363 \u2014 Canine CRP Biomarker Review","text":"<p>Description: Review of C-reactive protein (CRP) as a disease biomarker in dogs. Error Type: Gene vs. biomarker ambiguity. Explanation: CRP is encoded by a gene, but the paper focused on protein-level clinical measurements rather than genetic investigation.</p>"},{"location":"research/DISCUSSION/#case-s113-pmid-39620500-cyanobacteria-based-medical-applications","title":"Case S1.1.3: PMID 39620500 \u2014 Cyanobacteria-Based Medical Applications","text":"<p>Description: Engineering cyanobacteria to express human genes for medical use. Error Type: Species\u2013gene misattribution. Explanation: Human genes were incorrectly attributed to cyanobacteria, illustrating the difficulty of multi-organism genetic engineering contexts.</p>"},{"location":"research/DISCUSSION/#case-s114-pmid-39625374-canine-endocrinology","title":"Case S1.1.4: PMID 39625374 \u2014 Canine Endocrinology","text":"<p>Description: Study of parathyroid hormone (PTH) in canine endocrine disorders. Error Type: Gene vs. gene product ambiguity. Explanation: The focus was physiological rather than genetic, despite the presence of gene-encoded peptides.</p>"},{"location":"research/DISCUSSION/#s12-common-error-patterns-in-custom-biomni","title":"S1.2 Common Error Patterns in Custom Biomni","text":"<p>Across these cases, two dominant error modes were observed:</p> <ol> <li>Confusion between genes and gene products (biomarkers, hormones, peptides)</li> <li>Incomplete filtering of model organism involvement in multi-species studies</li> </ol> <p>These errors highlight semantic boundaries that remain challenging even for expert human curators.</p>"},{"location":"research/DISCUSSION/#supplementary-note-2-default-biomni-false-positive-breakdown-15-cases","title":"Supplementary Note 2: Default Biomni False Positive Breakdown (15 cases)","text":"<p>Default Biomni\u2019s false positives exhibited more systematic classification issues.</p>"},{"location":"research/DISCUSSION/#s21-no-genetic-content-6-cases","title":"S2.1 No Genetic Content (6 cases)","text":"<p>Papers incorrectly classified as genetic research despite lacking gene-focused investigation, including methodological, ecological, or nutritional studies.</p>"},{"location":"research/DISCUSSION/#s22-model-organism-contamination-3-cases","title":"S2.2 Model Organism Contamination (3 cases)","text":"<p>Studies primarily focused on recognized model organisms (e.g., E. coli, tobacco, humans) that were insufficiently filtered.</p>"},{"location":"research/DISCUSSION/#s23-ambiguous-or-tangential-gene-mentions-2-cases","title":"S2.3 Ambiguous or Tangential Gene Mentions (2 cases)","text":"<p>Papers mentioning enzymes or genes incidentally within biochemical or cultivation contexts, without genetic analysis.</p>"},{"location":"research/DISCUSSION/#s24-other-cases-4-cases","title":"S2.4 Other Cases (4 cases)","text":"<p>Remaining errors spanned multiple categories and reflected inconsistent application of classification criteria.</p>"},{"location":"research/DISCUSSION/#supplementary-note-3-pubtator-false-positive-analysis-22-cases","title":"Supplementary Note 3: PubTator False Positive Analysis (22 cases)","text":""},{"location":"research/DISCUSSION/#s31-human-gene-misclassification-10-cases","title":"S3.1 Human Gene Misclassification (10 cases)","text":"<p>Human genetics papers incorrectly retained due to incidental mentions of non-model organisms.</p>"},{"location":"research/DISCUSSION/#s32-false-gene-annotations-10-cases","title":"S3.2 False Gene Annotations (10 cases)","text":"<p>Entities annotated as genes despite representing biomarkers, metabolites, or non-genetic concepts.</p>"},{"location":"research/DISCUSSION/#s33-irrelevant-papers-2-cases","title":"S3.3 Irrelevant Papers (2 cases)","text":"<p>Papers lacking substantive biological research content but spuriously annotated.</p>"},{"location":"research/DISCUSSION/#supplementary-note-4-comparative-error-statistics","title":"Supplementary Note 4: Comparative Error Statistics","text":"Error Type PubTator Default Biomni Custom Biomni No genetic content 2/22 6/15 0/4 Model organism contamination 10/22 3/15 2/4 False gene annotations 10/22 0/15 0/4 Gene vs. biomarker/peptide included 2/15 2/4 Species\u2013gene misattribution 0/22 2/15 1/4"},{"location":"research/DISCUSSION/#supplementary-note-5-false-negative-analysis","title":"Supplementary Note 5: False Negative Analysis","text":""},{"location":"research/DISCUSSION/#s51-papers-missed-by-custom-biomni-26-cases","title":"S5.1 Papers Missed by Custom Biomni (26 cases)","text":""},{"location":"research/DISCUSSION/#s511-missing-ncbi-gene-entries","title":"S5.1.1 Missing NCBI Gene Entries","text":"<p>Many legitimate non-model organism genetic studies lacked registered NCBI Gene IDs, preventing database-based validation.</p> <p>Representative examples include studies on insecticide resistance genes, bacterial functional genomics, and algal DNA repair genes.</p>"},{"location":"research/DISCUSSION/#s512-classification-boundary-ambiguity","title":"S5.1.2 Classification Boundary Ambiguity","text":"<p>Several missed papers occupied gray zones between genetics, taxonomy, ecology, or physiology, where expert judgment may vary.</p>"},{"location":"research/DISCUSSION/#s52-papers-missed-by-default-biomni-6-cases","title":"S5.2 Papers Missed by Default Biomni (6 cases)","text":"<p>Papers retrieved exclusively by Custom Biomni included studies on plants, fungi, and some domesticated animals, suggesting that Default Biomni may have applied overly conservative filtering in specific contexts.</p>"},{"location":"research/DISCUSSION/#supplementary-note-6-prompt-design-considerations","title":"Supplementary Note 6: Prompt Design Considerations","text":""},{"location":"research/DISCUSSION/#s61-over-constraint-in-custom-biomni","title":"S6.1 Over-Constraint in Custom Biomni","text":"<p>The instruction to always verify species\u2013gene consistency using NCBI databases disproportionately penalized non-model organism research, where gene annotation coverage is incomplete by definition.</p>"},{"location":"research/DISCUSSION/#s62-prompt-simplification-hypothesis","title":"S6.2 Prompt Simplification Hypothesis","text":"<p>Reducing prompt complexity may improve agent consistency by lowering cognitive load and allowing flexible reasoning when database evidence is unavailable.</p>"},{"location":"research/DISCUSSION/#supplementary-note-7-implications-for-future-system-design","title":"Supplementary Note 7: Implications for Future System Design","text":"<p>These supplementary analyses underscore that remaining errors arise primarily from semantic ambiguity, database incompleteness, and prompt design choices rather than fundamental model incapability. Addressing these factors represents a clear pathway toward further performance gains.</p>"},{"location":"research/LIMITATIONS/","title":"Limitations","text":""},{"location":"research/LIMITATIONS/#scope-and-evaluation-metrics","title":"Scope and Evaluation Metrics","text":"<p>This study focused on evaluating precision in constructing a corpus of genetic research articles on non-model organisms, reflecting the primary requirement of high-precision literature curation workflows. Other performance metrics, including recall, accuracy, and F1-score, were not comprehensively assessed.</p> <p>This limitation was driven by practical constraints. Establishing complete ground truth for all 531 papers would require extensive expert manual curation, which was not feasible within the project timeline. Consequently, we prioritized rigorous precision evaluation over broader metric coverage. As a result, we cannot make definitive claims regarding recall or overall classification accuracy, although preliminary insights into false negatives are discussed based on retrieval overlap patterns.</p>"},{"location":"research/LIMITATIONS/#entity-level-annotation-limitations","title":"Entity-Level Annotation Limitations","text":"<p>Manual curation primarily targeted paper-level classification (i.e., whether a paper constitutes non-model organism genetics research). While gene and species annotations were reviewed, entity-level validation was not performed with equivalent rigor. Some ambiguity therefore remains in the ground truth for entity annotations, and reported entity-level precision values should be interpreted as indicative rather than definitive.</p>"},{"location":"research/LIMITATIONS/#data-limitations","title":"Data Limitations","text":""},{"location":"research/LIMITATIONS/#temporal-sampling-bias","title":"Temporal Sampling Bias","text":"<p>The corpus was sampled from a narrow three-day window (December 1\u20133, 2024), introducing potential temporal bias. Publication patterns may vary seasonally, and related studies may cluster temporally. While a longer sampling window would provide more representative coverage, the selected window enabled computationally feasible method development. Importantly, all methods were evaluated on the identical corpus, preserving the validity of comparative analysis.</p>"},{"location":"research/LIMITATIONS/#sample-size","title":"Sample Size","text":"<p>The 531-paper corpus was sufficient for initial method comparison but limits statistical power, coverage of rare research subtypes, and taxonomic diversity. Larger-scale evaluations would be necessary to assess performance robustness, particularly for uncommon species and niche research areas.</p>"},{"location":"research/LIMITATIONS/#methodological-limitations","title":"Methodological Limitations","text":""},{"location":"research/LIMITATIONS/#ground-truth-ambiguity","title":"Ground Truth Ambiguity","text":"<p>Classifying \u201cnon-model organism genetics research\u201d involves subjective judgment, particularly for studies at the boundary between genetics and related domains (e.g., ecology or physiology). To ensure internal consistency, all evaluations were conducted by a single domain expert. However, formal inter-rater reliability assessment was not performed, which limits confidence in edge-case classifications.</p>"},{"location":"research/LIMITATIONS/#classification-framework","title":"Classification Framework","text":"<p>In the absence of established taxonomies for non-model organism genetics research, we developed an ad hoc six-category classification framework. While applied consistently across all methods, this framework has not been externally validated, and boundary cases may be inconsistently categorized.</p>"},{"location":"research/LIMITATIONS/#database-coverage-constraints","title":"Database Coverage Constraints","text":"<p>Custom Biomni relies on NCBI Gene and Taxonomy databases for validation. However, gene annotation for non-model organisms is often incomplete, creating a structural limitation. Strict database-based validation may therefore exclude legitimate genetic studies lacking registered gene entries, contributing to reduced recall. This paradox highlights how incomplete annotation of non-model organisms\u2014one of the motivations for this study\u2014also constrains database-driven approaches.</p>"},{"location":"research/LIMITATIONS/#performance-and-practical-limitations","title":"Performance and Practical Limitations","text":"<p>Custom Biomni required higher computational cost and processing time than baseline method. While acceptable for targeted, high-value curation tasks, these costs may limit applicability to large-scale or real-time monitoring. Batch-based processing further introduces engineering complexity, although structured output mechanisms mitigated data loss risks.</p> <p>Results are also model-specific, reflecting performance of GPT-4.1-mini at the time of evaluation. Performance, cost, and error characteristics may change with future model updates or alternative LLMs.</p>"},{"location":"research/LIMITATIONS/#generalizability","title":"Generalizability","text":"<p>This study is specific to non-model organism genetics literature. Generalization to other biological domains, model organism research, or non-biological fields remains untested. Moreover, the evaluation represents a snapshot of AI capabilities in 2025; rapid advances in LLM technology may alter both performance and limitations over time.</p> <p>Custom Biomni\u2019s performance is also sensitive to prompt design. Prompt optimization was not systematic, and small prompt variations may lead to different outcomes, limiting reproducibility.</p>"},{"location":"research/LIMITATIONS/#comparison-constraints","title":"Comparison Constraints","text":"<p>PubTator represents a free, pre-computed annotation baseline with no customization or per-query cost, whereas Biomni agents required computational cost and leverage modern LLMs. Despite these structural differences, PubTator remains the most practical large-scale automated baseline and therefore an appropriate comparator.</p> <p>\u201cDefault Biomni\u201d was defined as Biomni without custom database integration. Alternative default configurations or prompt designs may yield different results, although both Biomni variants shared similar base settings to enable fair comparison.</p>"},{"location":"research/LIMITATIONS/#summary","title":"Summary","text":"<p>These limitations reflect the exploratory nature of this study and highlight key challenges in automated curation for non-model organism genetics, including incomplete databases, subjective classification boundaries, and precision\u2013recall trade-offs. While these constraints limit generalization and recall assessment, they also delineate clear directions for future methodological refinement and large-scale validation.</p>"},{"location":"research/LIMITATIONS/#supplementary-note-extended-limitations-and-context","title":"Supplementary Note : Extended Limitations and Context","text":"<p>This supplementary note provides methodological details and extended context for the limitations briefly described in the main text. It is intended to support transparency and reproducibility without duplicating the main discussion.</p>"},{"location":"research/LIMITATIONS/#s1-evaluation-scope","title":"S1. Evaluation Scope","text":""},{"location":"research/LIMITATIONS/#precision-centered-design","title":"Precision-Centered Design","text":"<p>Evaluation focused exclusively on precision, reflecting the primary requirement of literature curation workflows. Comprehensive recall estimation was infeasible due to the lack of complete ground truth for the full 531-paper corpus. Preliminary insights into false negatives were derived from retrieval overlap patterns but are not quantitatively reported.</p>"},{"location":"research/LIMITATIONS/#entity-level-annotation-uncertainty","title":"Entity-Level Annotation Uncertainty","text":"<p>Manual curation prioritized paper-level classification. Gene and species annotations were reviewed but not exhaustively curated, particularly for non-model organisms with inconsistent nomenclature. Reported entity-level precision should therefore be interpreted as indicative.</p>"},{"location":"research/LIMITATIONS/#s2-data-and-sampling-constraints","title":"S2. Data and Sampling Constraints","text":""},{"location":"research/LIMITATIONS/#temporal-window","title":"Temporal Window","text":"<p>The corpus was sampled from a three-day publication window (December 1\u20133, 2024), which may introduce temporal bias and topic clustering. The window was chosen for computational feasibility, and all methods were evaluated on the identical corpus, preserving valid relative comparison.</p>"},{"location":"research/LIMITATIONS/#corpus-size","title":"Corpus Size","text":"<p>The 531-paper corpus is sufficient to detect large performance differences but limits statistical power and taxonomic coverage. Larger, temporally diverse corpora are needed for robust evaluation.</p>"},{"location":"research/LIMITATIONS/#s3-methodological-considerations","title":"S3. Methodological Considerations","text":""},{"location":"research/LIMITATIONS/#ground-truth-subjectivity","title":"Ground Truth Subjectivity","text":"<p>Classification of non-model organism genetics research involves subjective boundary cases (e.g., genetic markers vs. genetics-focused studies). All annotations were performed by a single expert curator, and no formal inter-rater reliability analysis was conducted.</p>"},{"location":"research/LIMITATIONS/#classification-framework_1","title":"Classification Framework","text":"<p>A six-category, ad hoc classification framework was developed in the absence of standardized taxonomies. While consistently applied, it has not been externally validated and may be ambiguous for multi-category papers.</p>"},{"location":"research/LIMITATIONS/#database-coverage-paradox","title":"Database Coverage Paradox","text":"<p>Custom Biomni relies on NCBI Gene and Taxonomy validation, which is incomplete for non-model organisms. This improves precision but likely reduces recall by excluding legitimate studies lacking database support.</p>"},{"location":"research/LIMITATIONS/#s4-computational-and-generalization-limits","title":"S4. Computational and Generalization Limits","text":""},{"location":"research/LIMITATIONS/#cost-and-reliability","title":"Cost and Reliability","text":"<p>Custom Biomni incurs higher computational cost and requires batch-based processing, increasing engineering complexity. These constraints may limit applicability to large-scale or real-time monitoring.</p>"},{"location":"research/LIMITATIONS/#model-and-prompt-dependency","title":"Model and Prompt Dependency","text":"<p>Results are specific to GPT-4.1-mini and to the chosen prompt design. Model updates or prompt variations may alter performance, and transferability to other domains remains untested.</p>"},{"location":"research/LIMITATIONS/#supplementary-summary","title":"Supplementary Summary","text":"<p>These limitations reflect structural challenges in non-model organism literature and early-stage AI agent development. While they constrain absolute performance claims, they do not affect the validity of comparative conclusions, which are based on consistent evaluation across identical corpora.</p>"},{"location":"research/METHODS/","title":"Methods","text":""},{"location":"research/METHODS/#overview","title":"Overview","text":"<p>This study compares three approaches for constructing a high-precision corpus of genetic research articles on non-model organisms: (1) PubTator-based annotation filtering, (2) a Biomni AI agent with default configuration, and (3) a Biomni AI agent with custom enhancements. We evaluated these methods on a corpus of open-access journal articles published over a three-day period, assessing their precision in identifying relevant literature.</p>"},{"location":"research/METHODS/#operational-definitions","title":"Operational Definitions","text":""},{"location":"research/METHODS/#model-vs-non-model-organisms","title":"Model vs. Non-Model Organisms","text":"<p>Model Organism Definition: We operationally defined model organisms as the top 20 most frequently studied species in genetic research, based on annotation frequency in genome editing meta-database (https://doi.org/10.1016/j.ggedit.2022.100024).</p> <p>Data Source: Genome editing meta-database (<code>data/20251008_ge_metadata_all.csv</code>) (downloaded from https://github.com/szktkyk/gem_api)</p> <p>Selection Criteria: - Analyzed species frequency in studies using genome editing tools since 2000 - Top 20 species classified as \"model organisms\" - All other species classified as \"non-model organisms\" - Implementation: <code>scripts/select_model_species.py</code></p> <p>Coverage Analysis: - The top 20 model organisms accounted for 92.21% of all papers in the database - Results documented in <code>config/top20_organisms_with_taxid.csv</code> - Visualization available in <code>figures/model_species/top20_organisms_bar_chart.png</code> - coverage statistics available in <code>figures/model_species/coverage_statistics.txt</code></p> <p>Reference Sources: - NIH Model Organisms FAQ: https://public.csr.nih.gov/FAQs/ReviewersFAQs/ModelOrganisms - Howe et al. (2017). The Model Organism as a System: Integrating 'Omics' Data Sets. BMC Biology, 15:45. https://doi.org/10.1186/s12915-017-0391-5</p>"},{"location":"research/METHODS/#genetics-research-classification","title":"Genetics Research Classification","text":"<p>Scope Definition: The scope of this study was operationally defined to focus on genetic research conducted within non-model organisms. Given the lack of a widely accepted definition of \u201cnon-model organism genetics research\u201d in the literature, we adopted a pragmatic scope tailored to the objectives of this analysis. Specifically, we excluded studies in which non-model organisms were primarily investigated in the context of their effects on model organism genomes or biology (e.g., bacterial effects on human genomes), as such studies are centered on model organisms rather than on the genetics of non-model species themselves.</p> <p>*Classification Framework: In the absence of standardized taxonomies for classifying genetic research in non-model organisms, we developed a six-category classification framework to capture the major types of genetic studies observed in the literature. </p> <p>The six genetics research categories used in this study are as follows:</p> <ol> <li>Genomic Sequencing &amp; Identification: Genome sequencing and novel gene identification</li> <li>Comparative Analysis &amp; Annotation: Ortholog/homolog analysis for functional inference through sequence homology</li> <li>Gene Expression Profiling: Expression analysis across conditions, tissues, or developmental stages</li> <li>Phylogenetic and Evolutionary Analysis: Gene family expansion/contraction and evolutionary adaptation signatures</li> <li>Functional Validation &amp; Bioengineering: Experimental validation using techniques such as CRISPR or RNAi</li> <li>Methodological Development &amp; Diagnostics: Development of genetic diagnostic techniques and methodological advances</li> </ol> <p>Inclusion Criteria: Papers satisfying at least one of the above six categories were classified as genetics research.</p> <p>Note: This classification system represents an initial framework. More refined categorization may be possible upon completion of comprehensive literature annotation.</p>"},{"location":"research/METHODS/#data-collection","title":"Data Collection","text":""},{"location":"research/METHODS/#literature-corpus","title":"Literature Corpus","text":"<p>Search Query: <code>all[filter] AND pubmed pmc open access[filter] AND journal article[pt]</code></p> <p>Date Range: December 1-3, 2024 (3-day window)</p> <p>Initial Corpus Size: 4,959 articles </p> <p>Limitation: The narrow 3-day sampling window represents a methodological limitation. Ideally, a 1-month window with random sampling would provide more representative coverage. See LIMITATIONS.md for detailed discussion.</p>"},{"location":"research/METHODS/#workflow-for-ai-agent-input-selection","title":"Workflow for AI Agent Input Selection","text":"<p>Implementation: <code>wf_pre_agent.py</code></p> <p>Configuration Parameters: <pre><code>config = WorkflowConfig(\n    date_start=\"20241201\",\n    date_end=\"20241203\",\n    days_per_chunk_step1=1,\n    chunk_size_step2=900,\n    chunk_size_step3=400,\n    max_retries=3,\n    retry_delay=10,\n    model_species_config=\"config/top20_organisms_with_taxid.csv\",\n    output_dir=\"data/pre_agent\"\n)\n</code></pre></p> <p>Filtering Criteria: - Used PubTator annotations on titles and abstracts - Selected papers where the most frequently mentioned species was a non-model organism - Filtered Corpus Size: 531 papers</p> <p>Quality Control: - Manual validation of <code>main()</code> function - Unit tests implemented in <code>test_wf_pre_agent.py</code> - Venn diagram visualization generated using <code>create_venn_diagram.py</code> (output: <code>figures/wf_filtering/</code>)</p>"},{"location":"research/METHODS/#workflow-for-pubtator-baseline","title":"Workflow for PubTator Baseline","text":"<p>Implementation: <code>filter_pubtator_annotations.py</code></p> <p>Input: 531-paper corpus from AI agent workflow</p> <p>Filtering Criteria: 1. Retrieved all PubTator annotations (species and genes) from titles and abstracts 2. Excluded papers with any model organism annotations 3. Excluded papers lacking both species and gene annotations</p> <p>Filtered Corpus Size: 55 papers (10.4% of 531-paper corpus)</p> <p>Stability Check: Replication in November 2024 yielded identical 55-paper result, confirming reproducibility.</p>"},{"location":"research/METHODS/#experimental-setup","title":"Experimental Setup","text":""},{"location":"research/METHODS/#pubtator-baseline","title":"PubTator Baseline","text":"<p>Approach: Rule-based filtering using existing PubTator annotations</p> <p>Rationale: Establishes baseline performance of current state-of-the-art automated annotation systems</p> <p>Process: 1. Extract species and gene annotations from PubTator API 2. Apply filtering rules (exclude model organisms, require gene annotations) 3. No manual intervention or AI-based decision making</p>"},{"location":"research/METHODS/#default-biomni-agent","title":"Default Biomni Agent","text":"<p>Framework: Biomni (https://biomni.stanford.edu/) with CodeAct architecture</p> <p>Model: GPT-4.1-mini (selected over GPT-5-mini due to 20-40 second latency reduction)</p> <p>Tools Available: Standard Biomni toolkit for literature analysis and database queries</p> <p>Limitations of Batch Processing: - Initial attempts to process all 531 papers in a single agent session failed - CodeAct architecture generated loop-based code that applied uniform processing to all PMIDs - This prevented the desired \"organic processing\" (identify gene \u2192 query database \u2192 validate \u2192 compare with species \u2192 query alternative candidates) - Solution: Implemented paper-by-paper processing approach</p>"},{"location":"research/METHODS/#custom-biomni-agent","title":"Custom Biomni Agent","text":"<p>Enhancements over Default Biomni:</p>"},{"location":"research/METHODS/#1-mcp-server-integration","title":"1. MCP Server Integration","text":"<p>NCBI Gene Database Access (Enhanced Entity Linking): - Query NCBI Gene database for gene information - Retrieve gene names, IDs, and synonyms - Validate consistency by cross-referencing gene name with species taxonomy ID</p> <p>NCBI Taxonomy Database Access (Enhanced Entity Linking): - Query NCBI Taxonomy database for species information - Retrieve taxonomy IDs, scientific names, and taxonomic classifications (class level) - Enable precise species identification and validation</p> <p>Structured Data Generation (Robust Data Capture): - Incrementally append validated annotations to structured output files - Ensure data persistence in case of agent interruption - Enable resumable processing for large batches</p>"},{"location":"research/METHODS/#2-iterative-development-and-validation","title":"2. Iterative Development and Validation","text":"<p>Conducted six rounds of agent behavior validation and prompt refinement: - [Trial 1-6 documentation links preserved for internal reference] - Final prompt design and paper-level processing approach determined in Trial 6 - Implementation: <code>run_agent_20251030.py</code></p>"},{"location":"research/METHODS/#3-batch-processing-architecture","title":"3. Batch Processing Architecture","text":"<p>Challenge: Memory accumulation from repeated <code>agent.go()</code> calls - Accumulated message history and MCP connections - All log_messages retained in memory - Memory exhaustion (OOM Killer) after ~76 papers</p> <p>Solution: Subprocess-based batch processing - Implementation: <code>run_agent_batch_subprocess.py</code> (orchestrator) + <code>run_agent_worker.py</code> (worker) - Process papers in small batches with agent restart between batches - Achieved stable processing of 419 papers without interruption</p>"},{"location":"research/METHODS/#performance-and-cost-analysis","title":"Performance and Cost Analysis","text":"<p>Processing Volume: 419 papers (manual termination; system remained stable)</p> <p>Results: 62 papers identified as non-model organism genetics research (14.8% hit rate)</p> <p>Computational Cost (300-paper subset): - Total Cost: $3.76 - Total Processing Time: 11,182.65 seconds (3.11 hours) - Average Time per Paper: 37.3 seconds - Average Cost per Paper: $0.0125</p>"},{"location":"research/METHODS/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"research/METHODS/#precision","title":"Precision","text":"<p>Definition: $$\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives + False Positives}}$$</p> <p>Rationale for Focus on Precision: - For this project, minimizing false positives (irrelevant papers incorrectly classified as relevant) has been mostly paid attention - High precision ensures curated sets contain primarily relevant papers - Recall (sensitivity) and F1-score evaluation deferred to future work</p> <p>Ground Truth Establishment: Manual curation of sampled papers to establish true positive classifications</p> <p>Evaluation Scope: This study reports precision only. Comprehensive evaluation including accuracy, recall, and F1-score remains for future investigation (see LIMITATIONS.md).</p>"},{"location":"research/METHODS/#implementation-details","title":"Implementation Details","text":"<p>Programming Language: Python 3.11.14</p> <p>Key Dependencies: - Biomni framework - OpenAI API (GPT-4.1-mini) - NCBI E-utilities - PubTator API</p> <p>Code Availability: - Workflow implementations: <code>wf_pre_agent.py</code>, <code>filter_pubtator_annotations.py</code> - Agent implementations: <code>run_agent_batch_subprocess.py</code>, <code>run_agent_worker.py</code> - Analysis scripts: <code>scripts/select_model_species.py</code>, <code>scripts/check_species_match.py</code> - Configuration files: <code>config/top20_organisms_with_taxid.csv</code>, <code>config/mcp_config.yaml</code></p> <p>Docker Environment: - Dockerfile: <code>setup/Dockerfile</code> - Environment specifications: <code>setup/environment.yml</code>, <code>setup/customized_bio_env.yml</code></p> <p>Reproducibility: All code, configurations, and processed data are available in this repository to enable full reproduction of results.</p> <p>Default Biomni Configuration: In this study, Default Biomni refers to a configuration in which a subset of the standard Biomni tools was manually selected based on their relevance to the target task. This setting reflects a realistic, task-oriented use of Biomni rather than an unconstrained or fully enabled tool suite, and was intended to provide a fair and practical baseline for comparison.</p>"},{"location":"research/RESULTS/","title":"Results","text":""},{"location":"research/RESULTS/#overview","title":"Overview","text":"<p>We compared three approaches for constructing a high-precision corpus of genetic research articles on non-model organisms using a pre-filtered set of 531 articles. All methods were applied to the same input corpus to ensure a direct and fair comparison. Evaluation focused on precision, reflecting the primary requirement of literature curation workflows, where minimizing false positives is critical due to the substantial downstream costs of manual review.</p>"},{"location":"research/RESULTS/#performance-comparison","title":"Performance Comparison","text":""},{"location":"research/RESULTS/#overall-precision-and-retrieval-characteristics","title":"Overall Precision and Retrieval Characteristics","text":"<p>Table 1 summarizes the performance of the three approaches. Custom Biomni achieved the highest precision (90.91%), outperforming Default Biomni (80.00%) and PubTator (60.00%).</p> Metric Default Biomni Custom Biomni PubTator Input corpus size 531 531 531 Papers retrieved 75 44 55 Precision 80.00% 90.91% 60.00% Gene annotation precision 59.72% 71.11% 38.97% Species annotation precision 63.98% 83.33% 65.55% <p>A clear retrieval volume\u2013precision trade-off was observed. Default Biomni retrieved the largest number of papers but at lower precision, whereas Custom Biomni retrieved fewer papers while maintaining substantially higher precision. PubTator showed intermediate retrieval volume but the lowest precision overall.</p>"},{"location":"research/RESULTS/#entity-annotation-performance","title":"Entity Annotation Performance","text":"<p>Custom Biomni consistently outperformed the other methods in both gene and species annotation precision. In particular, gene annotation precision for PubTator was markedly low (38.97%), substantially lagging behind both Biomni variants. This gap was less pronounced for species annotations, where PubTator performed comparably to Default Biomni.</p> <p>These results support our hypothesis that gene annotation for non-model organisms remains a major limitation of existing automated annotation systems, whereas species recognition is relatively robust.</p>"},{"location":"research/RESULTS/#error-analysis","title":"Error Analysis","text":""},{"location":"research/RESULTS/#false-positives","title":"False Positives","text":"<p>Analysis of false positives revealed distinct error modes across methods:</p> <ul> <li>PubTator frequently admitted papers with generic or incidental gene mentions, contamination from model organism\u2013centric studies, or non-genetic research falsely flagged by gene annotations.</li> <li>Default Biomni errors were primarily due to overgeneralization and insufficient contextual discrimination between primary research focus and background mentions.</li> <li>Custom Biomni produced the fewest false positives, mostly limited to edge cases involving complex multi-organism studies or borderline definitions of genetics research.</li> </ul> <p>The reduction in false positives for Custom Biomni can be attributed to its integration of external validation using NCBI Gene and Taxonomy resources.</p>"},{"location":"research/RESULTS/#computational-efficiency","title":"Computational Efficiency","text":""},{"location":"research/RESULTS/#cost-and-time-considerations","title":"Cost and Time Considerations","text":"<p>Both Biomni approaches required modest computational resources. Custom Biomni incurred slightly higher cost and processing time than Default Biomni, reflecting its additional validation steps.</p> Method Total time Total cost Cost per true positive Custom Biomni 6.09 h $6.76 $0.169 Default Biomni 5.28 h $6.01 $0.100 PubTator &lt;5 min $0 $0* <ul> <li>PubTator requires substantial downstream manual filtering due to lower precision.</li> </ul> <p>Although Custom Biomni is more expensive per retrieved paper, its higher precision substantially reduces downstream curation effort, suggesting better overall cost-effectiveness for large-scale literature curation.</p>"},{"location":"research/RESULTS/#scalability","title":"Scalability","text":"<p>Both Biomni methods demonstrated stable performance across the evaluated corpus. Extrapolation to larger datasets (e.g., 10,000 articles) suggests feasible runtimes (approximately 4\u20135 days) and moderate costs, supporting practical scalability for large literature collections.</p>"},{"location":"research/RESULTS/#summary-of-key-findings","title":"Summary of Key Findings","text":"<ol> <li>Custom Biomni achieved the highest precision (90.91%), substantially reducing false positives.</li> <li>PubTator exhibited particularly weak gene annotation performance for non-model organisms.</li> <li>Higher precision corresponded to lower retrieval volume, highlighting an inherent trade-off.</li> <li>Improved annotation precision translated into meaningful reductions in downstream curation cost.</li> <li>AI agent\u2013based approaches with database-aware validation offer a promising direction for high-precision literature curation in understudied biological domains.</li> </ol>"}]}